{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW04: Supervised Machine Learning, Regression and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-process Text\n",
    "We do sentiment analysis on the [Movie Review Data](https://www.cs.cornell.edu/people/pabo/movie-review-data/). If you would like to know more about the data, have a look at [the paper](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf) (but no need to do so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘scale_data.tar.gz’ already there; not retrieving.\n",
      "\n",
      "File ‘scale_whole_review.tar.gz’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this tutorial, we do sentiment analysis\n",
    "# download the data\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar xf aclImdb_v1.tar.gz\n",
    "\n",
    "!wget -nc https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
    "!wget -nc https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
    " \n",
    "!tar xf scale_data.tar.gz \n",
    "!tar xf scale_whole_review.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load the data for which we provide the function below. Note how we also preprocess the text using gensim's simple_preprocess() function and how we already split the data into a train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wasn't me:\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "def load_data():\n",
    "    examples, labels = [], []\n",
    "    authors = os.listdir(\"scale_whole_review\")\n",
    "    for author in authors:\n",
    "        path = os.listdir(os.path.join(\"scale_whole_review\", author, \"txt.parag\"))\n",
    "        fn_ids = os.path.join(\"scaledata\", author, \"id.\" + author)\n",
    "        fn_ratings = os.path.join(\"scaledata\", author, \"rating.\" + author)\n",
    "        with open(fn_ids) as ids, open(fn_ratings) as ratings:\n",
    "            for idx, rating in zip(ids, ratings):\n",
    "                labels.append(float(rating.strip()))\n",
    "                filename_text = os.path.join(\"scale_whole_review\", author, \"txt.parag\", idx.strip() + \".txt\")\n",
    "                with open(filename_text, encoding='latin-1') as f:\n",
    "                    examples.append(\" \".join(simple_preprocess(f.read())))\n",
    "    return examples, labels\n",
    "                  \n",
    "X,y  = load_data()\n",
    "\n",
    "assert all((0 <= y <= 1) for y in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: bloody child the director writer cinematographer nina menkes screenwriter tinka menkes editors nina  ...\n",
      "label: 0.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print (\"text:\", X_train[0][0:100], '...')\n",
    "print(\"label:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a TF_IDF Vectorizer on X_train and vectorize X_train and X_test\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                        max_df=.5,  \n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "## train vectorizer\n",
    "vec.fit(X_train)\n",
    "\n",
    "## transform X_train to TF-IDF values\n",
    "X_train_tfidf = vec.transform(X_train)\n",
    "## transform X_test to TF-IDF values\n",
    "X_test_tfidf = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale both training and test data with the standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# The StandardScaler normalizes columnwise:\n",
    "assert np.isclose(1, np.var(StandardScaler().fit_transform(np.random.random(size=(4, 4))), axis=0)).all()\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train an elastic net on the transformed output of the scaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet(alpha=0.01)\n",
    "\n",
    "## train the ElasticNet\n",
    "en.fit(X_train_tfidf, y_train)\n",
    "\n",
    "## predict the testset\n",
    "y_pred = en.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error    0.016355\n",
      "r2_score              0.483110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, balanced_accuracy_score\n",
    "## print mean squared error and r2 score on the test set\n",
    "\n",
    "print(pd.Series({f.__name__: f(y_test, y_pred) for f in [mean_squared_error, r2_score]}).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train an OLS model doing binary prediction on these movie reviews. Two get two bins, we transform the continuous ratings into two classes, where one class contains all the negative ratings (value < 0.5), the other class all the positive ratings (value > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.round(y_train)\n",
    "y_test = np.round(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80%\n",
      "Predictors:\n",
      "powerful        0.246628\n",
      "easy            0.216025\n",
      "solid           0.215400\n",
      "speaks          0.197862\n",
      "breathtaking    0.197833\n",
      "equal           0.196745\n",
      "honest          0.195792\n",
      "delightful      0.195505\n",
      "technique       0.195227\n",
      "great           0.194487\n"
     ]
    }
   ],
   "source": [
    "## train logistic regression on X_train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "## train a logistic regression\n",
    "logistic_regression.fit(X_train_tfidf, y_train)\n",
    "\n",
    "## predict the testset \n",
    "y_pred = (logistic_regression.predict(X_test_tfidf) >= 0.5).astype(float)\n",
    "\n",
    "# ##since we have continuous output, we need to post-process our labels into two classes. We choose a threshold of 0.5 \n",
    "# def map_predictions(predicted):\n",
    "#     predicted = [1 if i > 0.5 else 0 for i in predicted]\n",
    "#     return predicted\n",
    "\n",
    "## print the accuracy of our classifier on the testset\n",
    "print(\"Accuracy: {:.2g}%\".format(100 * accuracy_score(y_test, y_pred)))\n",
    "\n",
    "## print the 10 most informative words of the regression (the 10 words having the highest coefficients)\n",
    "predictors = pd.Series(index=vec.get_feature_names(), data=logistic_regression.coef_.flatten()).nlargest(10)\n",
    "print(\"Predictors:\")\n",
    "print(predictors.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we train an XGBoost classifier to do topic prediction on the AG news dataset, which is a multi-class prediction problem (4 classes). We again have to vectorize the data, train the classifier, predict the testset and output an evaluation metric (we go for accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters)</td>\n",
       "      <td>Reuters - Stocks ended slightly higher on Frid...</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "1      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "2      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "3      3  Oil prices soar to all-time record, posing new...   \n",
       "4      3        Stocks End Up, But Near Year Lows (Reuters)   \n",
       "\n",
       "                                                lead  \\\n",
       "0  Reuters - Private investment firm Carlyle Grou...   \n",
       "1  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "2  Reuters - Authorities have halted oil export\\f...   \n",
       "3  AFP - Tearaway world oil prices, toppling reco...   \n",
       "4  Reuters - Stocks ended slightly higher on Frid...   \n",
       "\n",
       "                                                text  \n",
       "0  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "1  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "2  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "3  Oil prices soar to all-time record, posing new...  \n",
       "4  Stocks End Up, But Near Year Lows (Reuters) Re...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.zip')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# only consider 10% of the data\n",
    "dfs = df.sample(frac=0.1)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfs[\"text\"], dfs[\"label\"], test_size=0.33, random_state=42)\n",
    "\n",
    "vec = TfidfVectorizer(min_df=5, # at min 1% of docs\n",
    "                        max_df=.5,  \n",
    "                        stop_words='english',\n",
    "                        max_features=2000,\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "# transform into TF-IDF values\n",
    "X_train_tfidf = vec.fit_transform(X_train).todense()\n",
    "X_test_tfidf = vec.transform(X_test).todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost provides an interface to SKLearn classifiers, e.g. they implement the same train and predict methods as an SKLearn classifier would. If you are interested in a more detailed overview, have a look at the [official documentation](https://xgboost.readthedocs.io/en/latest/python/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBModel(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "         colsample_bynode=1, colsample_bytree=1, eval_metric='mlogloss',\n",
       "         gamma=0, gpu_id=-1, interaction_constraints='',\n",
       "         learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "         min_child_weight=1, monotone_constraints='()', n_estimators=25,\n",
       "         n_jobs=8, num_class=5, num_parallel_tree=1, objective='multi:softmax',\n",
       "         random_state=0, reg_alpha=0, reg_lambda=1, subsample=1,\n",
       "         tree_method='exact', validate_parameters=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = {'objective': 'multi:softmax', 'num_class': 5, 'n_estimators': 25, 'eval_metric': 'mlogloss'}\n",
    "\n",
    "# note how we only have 4 labels, but we need to pass \"num_class\": 5\n",
    "# if we pass \"num_class\": 4, we get the error \"label must be in [0, num_class).\"\n",
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBModel(**param_dict)\n",
    "\n",
    "## train the XGBModel \n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8042929292929293\n"
     ]
    }
   ],
   "source": [
    "## predict the testset \n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "## evaluate the predictions using accuracy as a metric\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "language": "python",
   "name": "python38564bitvenvb5ce6a286a0a4e09a12406ae40e03074"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
