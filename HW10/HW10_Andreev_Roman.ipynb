{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "congressional-subsection",
   "metadata": {},
   "source": [
    "# HW10: A Simple Chatbot using GPT2\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can <span style=\"color:red\">not</span> skip one section this homework.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-billion",
   "metadata": {},
   "source": [
    "**Training a Chatbot**\n",
    "\n",
    "In this exercise, we are going to train a simple chatbot based on DistilGPT2. Find an overview of the GPT2 architecture in hugggingface [here](https://huggingface.co/transformers/model_doc/gpt2.html). We will use the [CCPE data](https://www.aclweb.org/anthology/W19-5941.pdf) (no need to read the paper for this exercise, we provide data loading utilties). The dataset offers exciting possibilities to train sophisticated chatbots, however we only explore a very simple version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "saving-bangkok",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'ccpe'...\n",
      "remote: Enumerating objects: 13, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
      "remote: Total 13 (delta 3), reused 6 (delta 2), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (13/13), 531.27 KiB | 171.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# clone the github repo\n",
    "!git clone https://github.com/google-research-datasets/ccpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from more_itertools import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "precise-silly",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('generally speaking what type of movies do you watch', 'I like thrillers a lot.')\n('I like thrillers a lot.', 'thrillers? for example?')\n('thrillers? for example?', \"Zodiac's one of my favorite movies.\")\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    with open(\"ccpe/data.json\") as f:\n",
    "        for conversation in json.load(f):\n",
    "            for (a, b) in pairwise(conversation[\"utterances\"]):\n",
    "                yield (a['text'], b['text'])\n",
    "\n",
    "data = list(load_data())\n",
    "\n",
    "print(*data[0:3], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "716\n[['generally speaking what type of movies do you watch'\n  'I like thrillers a lot.']\n ['I like thrillers a lot.' 'thrillers? for example?']\n ['thrillers? for example?' \"Zodiac's one of my favorite movies.\"]\n [\"Zodiac's one of my favorite movies.\"\n  \"Zodiac the movie about a serial killer from the '60s or '70s, around there.\"]\n [\"Zodiac the movie about a serial killer from the '60s or '70s, around there.\"\n  'Zodiac? oh wow ok, what do you like about that movie']]\n"
     ]
    }
   ],
   "source": [
    "data = np.reshape(data[:-13], (-1, 16, 2))\n",
    "print(len(data))\n",
    "print(data[0][:5])\n",
    "\n",
    "\n",
    "# please note how we arrange the data in pairs of (previous_sentence, current_sentence)\n",
    "# and each batch contains 16 such sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-marina",
   "metadata": {},
   "source": [
    "**The data**\n",
    "As we can see from the data, we only extract sentence pairs and will train models on these pairs in isolation. This is a very simplified version of training a chatbot. \n",
    "\n",
    "**In a more realistic setting**, we would include more conversation history, perhaps have to retrieve additional information from a fact base to generate factually accurate examples (e.g. think about a chatbot which could suggest restaurants in a city and needs to have a list available of all restaurants in that city). We would probably also encode the speaker and the chatbot, and guess the speech act to give the desired response (e.g. if a speaker just wants to do small talk, we would guess this and reply accordingly. If we guess he is asking for factual information, we should structure our response very differently. However, in this exercise we stick to our very simplified version.\n",
    "\n",
    "We have already prepared the data such that it comes in batches of 32 examples each.\n",
    "\n",
    "To train a chatbot, we need data, a tokenizer, a model and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pretty-diagram",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token_id=0)\n",
    "\n",
    "##because GPT2 tokenizers do not have padding, cls and sep tokens, we have to add these ourselves\n",
    "##we won't need the # character, so this will be the pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '#'})\n",
    "tokenizer.add_special_tokens({'cls_token': 'bos'})\n",
    "tokenizer.add_special_tokens({'sep_token': 'bos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "## load model\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# https://huggingface.co/transformers/model_doc/gpt2.html\n",
    "embedding_layer = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n",
    "#model.set_num_special_tokens(len(SPECIAL_TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "blessed-activation",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "sample_output:\n",
      "tf.Tensor(\n",
      "[[5195  466  345  588  428 1611  286 6918   30  198  198  198  198  198\n",
      "   198  198  198  198  198  198  198  198  198  198  198  198  198  198\n",
      "   198  198  198  198  198  198  198  198  198  198  198  198  198  198\n",
      "   198  198  198  198  198  198  198  198]], shape=(1, 50), dtype=int32)\n",
      "GENERATED:\n",
      "Why do you like this kind of movies?/////////////////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "##let's have a look what we generate before fine-tuning our chatbot\n",
    "\n",
    "input_ids = tokenizer.encode(\"Why do you like this kind of movies?\", return_tensors='tf')\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    ")\n",
    "\n",
    "print(\"sample_output:\")\n",
    "print(sample_output)\n",
    "\n",
    "print(\"GENERATED:\")\n",
    "print(tokenizer.decode(sample_output[0]).replace(\"\\n\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "colored-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement an optimizer with learning_rate = 2e-5 for all parameters\n",
    "\n",
    "learning_rate = 2e-5\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "refined-produce",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16, 2)\n"
     ]
    }
   ],
   "source": [
    "## train the model\n",
    "num_epochs = 1\n",
    "\n",
    "## training on all 716 batches takes roughly one hour, so we just train for 100 steps \n",
    "max_steps = 100\n",
    "\n",
    "for (i, batch) in enumerate((data[:max_steps])):\n",
    "    print(batch.shape)\n",
    "    print(tokenizer(batch[0, 0], return_tensors='tf'))\n",
    "    break\n",
    "    #tokenizer()\n",
    "\n",
    "    ## prepare model input\n",
    "    #In the textual entailment example in the notebook, we encode\n",
    "    #[CLS-token]premise[SEP-token]hypothesis[SEP-TOKEN]\n",
    "    #Here, we would like to encode \n",
    "    #[CLS-token]previous_sentence[Sep-token]current_sentence[SEP-TOKEN]\n",
    "    \n",
    "    ##Compute a forward step (the labels are simply the input_ids)\n",
    "    #since gpt-2 reads from left to right, it will predict the label at each timestep without having access to that token's information during training\n",
    "    \n",
    "    ##Compute a backward step\n",
    "    \n",
    "    ##Perform an optimzer step\n",
    "\n",
    "    ##Clear gradients of the optimizer\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's have a look what we generate after fine-tuning our chatbot\n",
    "\n",
    "input_ids = tokenizer.encode(\"Why do you like this kind of movies?\", return_tensors='tf')\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    ")\n",
    "\n",
    "tokenizer.decode(sample_output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38564bitvenvb5ce6a286a0a4e09a12406ae40e03074",
   "display_name": "Python 3.8.5 64-bit ('venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "88f42fa326d64d7433b6bf2a90e18fa8adc33f6e5abd7b8bafe6e492118b07cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}